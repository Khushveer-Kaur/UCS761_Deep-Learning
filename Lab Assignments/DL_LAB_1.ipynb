{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Setup and Dataset Definition**\n",
        "\n",
        "### The first step in deep learning is defining the task via data. A perceptron works on binary inputs ($x_1, x_2 \\in \\{0,1\\}$) and produces a binary output ($y \\in \\{0,1\\}$). We define the truth tables for various logic gates here. Note that \"XOR\" is included to demonstrate the limitations of a single-layer perceptron."
      ],
      "metadata": {
        "id": "WtCPnuI0ZIhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the datasets as list of tuples (x1, x2, target_y)\n",
        "# Each dataset defines the specific task the model must learn\n",
        "\n",
        "datasets = {\n",
        "    \"AND\":  [(0,0,0), (0,1,0), (1,0,0), (1,1,1)],\n",
        "    \"OR\":   [(0,0,0), (0,1,1), (1,0,1), (1,1,1)],\n",
        "    \"NAND\": [(0,0,1), (0,1,1), (1,0,1), (1,1,0)],\n",
        "    \"NOR\":  [(0,0,1), (0,1,0), (1,0,0), (1,1,0)],\n",
        "    \"XOR\":  [(0,0,0), (0,1,1), (1,0,1), (1,1,0)]\n",
        "}\n",
        "\n",
        "print(\"Datasets initialized for AND, OR, NAND, NOR, and XOR.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md2UExjFZVI9",
        "outputId": "bb744b3a-0633-45b0-f200-4195a60312ef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets initialized for AND, OR, NAND, NOR, and XOR.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Perceptron Decision Function**\n",
        "### This cell implements the \"Forward Pass.\" The perceptron calculates a score ($z$) by multiplying inputs by weights ($w$) and adding a bias ($b$).\n",
        "**1. Weights**: Represent the importance of each input.\n",
        "\n",
        "**2. Bias**: Represents the baseline tendency (strictness or leniency).\n",
        "\n",
        "**3. Activation**: A \"Step Function\" converts the score into a hard 0 or 1 decision."
      ],
      "metadata": {
        "id": "w4MNxa4nZeQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x1, x2, w1, w2, b):\n",
        "    # Calculate the weighted sum (Evidence Aggregation)\n",
        "    z = (w1 * x1) + (w2 * x2) + b\n",
        "\n",
        "    # Apply the Step Function (Decision Rule)\n",
        "    y_hat = 1 if z >= 0 else 0\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "q_V7Fb0KZxKe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Perceptron Learning Rule**\n",
        "### Learning is \"controlled decision adjustment\". When the model makes a mistake, we calculate an Error Signal ($y - \\hat{y}$).\n",
        "If error is 0, no update happens.\n",
        "\n",
        "If error exists, weights and bias are adjusted by the Learning Rate ($\\eta$).\n",
        "\n",
        "$\\eta$ controls the speed vs. stability trade-off; we use $0.1$ as suggested."
      ],
      "metadata": {
        "id": "ZP32smbwaByn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_perceptron(dataset, learning_rate=0.1, epochs=10):\n",
        "    # Initialize weights and bias to zero\n",
        "    w1, w2, b = 0.0, 0.0, 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for x1, x2, y in dataset:\n",
        "            # Get prediction\n",
        "            y_hat = predict(x1, x2, w1, w2, b)\n",
        "\n",
        "            # Calculate error\n",
        "            error = y - y_hat\n",
        "\n",
        "            # Update parameters only if error != 0\n",
        "            if error != 0:\n",
        "                w1 += learning_rate * error * x1\n",
        "                w2 += learning_rate * error * x2\n",
        "                b += learning_rate * error\n",
        "                total_error += abs(error)\n",
        "\n",
        "        # Stop if the model has converged (zero errors)\n",
        "        if total_error == 0:\n",
        "            break\n",
        "\n",
        "    return w1, w2, b"
      ],
      "metadata": {
        "id": "X6YeyrQLZl_C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Execution and Evaluation**\n",
        "### This final block executes the training for each gate. It prints the final weights and final bias as required by your lab task. We also perform a sanity check by testing the trained model on all four inputs to verify correct predictions."
      ],
      "metadata": {
        "id": "SxkW8ggfaXJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for gate_name, data in datasets.items():\n",
        "    # Train the model\n",
        "    final_w1, final_w2, final_b = train_perceptron(data)\n",
        "\n",
        "    print(f\"Results for {gate_name} Gate:\")\n",
        "    print(f\"  Final Parameters: w1={final_w1:.2f}, w2={final_w2:.2f}, b={final_b:.2f}\")\n",
        "\n",
        "    # Verification Step\n",
        "    correct = True\n",
        "    for x1, x2, y in data:\n",
        "        prediction = predict(x1, x2, final_w1, final_w2, final_b)\n",
        "        status = \"Correct\" if prediction == y else \"FAILED\"\n",
        "        if prediction != y: correct = False\n",
        "        print(f\"  Input: ({x1}, {x2}), Target: {y}, Predicted: {prediction}, Status: {status}\")\n",
        "\n",
        "    if gate_name == \"XOR\" and not correct:\n",
        "        print(\"  Note: XOR failed because it is not linearly separable.\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsl5w0FpadDm",
        "outputId": "06bc6959-e922-4f37-bd19-00e67b994f09"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for AND Gate:\n",
            "  Final Parameters: w1=0.20, w2=0.10, b=-0.20\n",
            "  Input: (0, 0), Target: 0, Predicted: 0, Status: Correct\n",
            "  Input: (0, 1), Target: 0, Predicted: 0, Status: Correct\n",
            "  Input: (1, 0), Target: 0, Predicted: 0, Status: Correct\n",
            "  Input: (1, 1), Target: 1, Predicted: 1, Status: Correct\n",
            "Results for OR Gate:\n",
            "  Final Parameters: w1=0.10, w2=0.10, b=-0.10\n",
            "  Input: (0, 0), Target: 0, Predicted: 0, Status: Correct\n",
            "  Input: (0, 1), Target: 1, Predicted: 1, Status: Correct\n",
            "  Input: (1, 0), Target: 1, Predicted: 1, Status: Correct\n",
            "  Input: (1, 1), Target: 1, Predicted: 1, Status: Correct\n",
            "Results for NAND Gate:\n",
            "  Final Parameters: w1=-0.20, w2=-0.10, b=0.20\n",
            "  Input: (0, 0), Target: 1, Predicted: 1, Status: Correct\n",
            "  Input: (0, 1), Target: 1, Predicted: 1, Status: Correct\n",
            "  Input: (1, 0), Target: 1, Predicted: 1, Status: Correct\n",
            "  Input: (1, 1), Target: 0, Predicted: 0, Status: Correct\n",
            "Results for NOR Gate:\n",
            "  Final Parameters: w1=-0.10, w2=-0.10, b=0.00\n",
            "  Input: (0, 0), Target: 1, Predicted: 1, Status: Correct\n",
            "  Input: (0, 1), Target: 0, Predicted: 0, Status: Correct\n",
            "  Input: (1, 0), Target: 0, Predicted: 0, Status: Correct\n",
            "  Input: (1, 1), Target: 0, Predicted: 0, Status: Correct\n",
            "Results for XOR Gate:\n",
            "  Final Parameters: w1=-0.10, w2=0.00, b=0.00\n",
            "  Input: (0, 0), Target: 0, Predicted: 1, Status: FAILED\n",
            "  Input: (0, 1), Target: 1, Predicted: 1, Status: Correct\n",
            "  Input: (1, 0), Target: 1, Predicted: 0, Status: FAILED\n",
            "  Input: (1, 1), Target: 0, Predicted: 0, Status: Correct\n",
            "  Note: XOR failed because it is not linearly separable.\n"
          ]
        }
      ]
    }
  ]
}