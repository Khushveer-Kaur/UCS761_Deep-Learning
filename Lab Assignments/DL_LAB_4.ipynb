{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment Setup and Data Loading**\n",
        "\n",
        "### This block imports libraries like `Pandas` for data manipulation and `NumPy` for mathematical operations. We load the dataset and inspect its structure to determine how many inputs (features) the model needs to handle."
      ],
      "metadata": {
        "id": "bRalikhUyBWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"multiple_linear_regression_dataset.csv\")\n",
        "\n",
        "# Inspect data\n",
        "print(\"First 5 rows:\\n\", data.head())\n",
        "print(\"\\nColumn Names:\", data.columns)\n",
        "print(\"Shape of Dataset:\", data.shape)\n",
        "\n",
        "# Think About This (Step 1):\n",
        "# - Inputs: 'age' and 'experience'\n",
        "# - Output: 'income'\n",
        "# - Features: The model needs to handle 2 features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ1_sL_szgjT",
        "outputId": "0993a349-3c72-4d20-bec4-cb9c818aedd7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows:\n",
            "    age  experience  income\n",
            "0   25           1   30450\n",
            "1   30           3   35670\n",
            "2   47           2   31580\n",
            "3   32           5   40130\n",
            "4   43          10   47830\n",
            "\n",
            "Column Names: Index(['age', 'experience', 'income'], dtype='object')\n",
            "Shape of Dataset: (20, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**\n",
        " ### We separate the dataset into independent variables ($X$) and the target variable ($y$). This mapping is essential because if inputs and outputs are mixed, the learning process has no meaning."
      ],
      "metadata": {
        "id": "jbZrZOUPyUA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs (features)\n",
        "X = data[[\"age\", \"experience\"]].values\n",
        "\n",
        "# Output (target)\n",
        "y = data[\"income\"].values\n",
        "\n",
        "# Think About This (Step 2):\n",
        "# - X has 2 columns because it represents 2 independent features\n",
        "# - y has 1 column because we are predicting a single numeric value"
      ],
      "metadata": {
        "id": "tl9IY2gU0LDz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parameter Initialization**\n",
        "### We initialize the weights and bias. Since learning starts from imperfect guesses, we begin with zeros. We need exactly one weight for every input feature so each input can have its own level of \"importance\"."
      ],
      "metadata": {
        "id": "AQ8ZgZ7uycgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of features\n",
        "n_features = X.shape[1]\n",
        "\n",
        "# Initialize weights (one per feature) and bias (baseline offset)\n",
        "w = np.zeros(n_features)\n",
        "b = 0.0\n",
        "\n",
        "# Think About This (Step 3):\n",
        "# - One weight per feature: Needed to calculate the specific importance of each input\n",
        "# - Bias: Separate as it represents the baseline salary offset when inputs are zero\n",
        "# - Large values: Risky because they might cause the loss to explode or slow down convergence"
      ],
      "metadata": {
        "id": "NG_EMEbZ0v2y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Forward Pass (Prediction)**\n",
        "### This defines the Linear Neuron. We use the formula $\\hat{y} = X \\cdot w + b$. There is no activation function (like Sigmoid) because we need to preserve numeric magnitude, not destroy it for a decision."
      ],
      "metadata": {
        "id": "EHrsIC1fytJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w, b):\n",
        "    # Returns predicted values: y_hat = w1x1 + w2x2 + b\n",
        "    y_hat = X.dot(w) + b\n",
        "    return y_hat\n",
        "\n",
        "# Think About This (Step 4):\n",
        "# - No activation: Because we need a continuous number, not a 0/1 category\n",
        "# - y_hat values: Can be any real number (continuous)\n",
        "# - vs Logistic Regression: Logistic uses a Sigmoid to squash output between 0 and 1"
      ],
      "metadata": {
        "id": "0_i8J_Gf09kI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss Function (MSE)**\n",
        "\n",
        "### We implement Mean Squared Error (MSE). This function summarizes how far our predictions are from the actual values."
      ],
      "metadata": {
        "id": "sNRr38rKy2lI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_error(y, y_hat):\n",
        "    # Calculates the average of the squared errors\n",
        "    loss = ((y_hat - y) ** 2).mean()\n",
        "    return loss\n",
        "\n",
        "# Think About This (Step 5):\n",
        "# - Why square?: To remove signs (penalize over/under-estimates equally) and amplify large errors\n",
        "# - Very wrong prediction: MSE will penalize it heavily due to the squaring\n",
        "# - Why not absolute error?: Squaring makes the function smooth, which is better for calculus/gradients"
      ],
      "metadata": {
        "id": "2oWrrUU51hOF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradient Computation**\n",
        "\n",
        "### Gradients tell the model how to adjust weights to reduce loss. They provide the direction and magnitude of the necessary change"
      ],
      "metadata": {
        "id": "_TpgyZYFzA8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradients(X, y, y_hat):\n",
        "    N = len(y)\n",
        "    # Gradient of MSE w.r.t weights: (2/N) * X^T * (error)\n",
        "    dw = (2/N) * X.T.dot(y_hat - y)\n",
        "    # Gradient of MSE w.r.t bias: (2/N) * sum(error)\n",
        "    db = (2/N) * (y_hat - y).sum()\n",
        "    return dw, db\n",
        "\n",
        "# Think About This (Step 6):\n",
        "# - X in dw: Because weights are tied to specific features; bias is an independent offset\n",
        "# - Error term: Appears because gradients are calculated based on the difference between prediction and truth\n",
        "# - Error is zero: Gradients become zero, meaning the model has stopped learning (it is perfect)"
      ],
      "metadata": {
        "id": "z0gSCijt1u_4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parameter Update (Gradient Descent)**\n",
        "### We update the parameters using the subtraction rule: $w = w - \\eta \\cdot dw$. The learning rate ($\\eta$) controls the speed of these updates."
      ],
      "metadata": {
        "id": "KHLt_ZNEzFnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(w, b, dw, db, lr):\n",
        "    # Move parameters in the opposite direction of the gradient to reduce loss\n",
        "    w = w - lr * dw\n",
        "    b = b - lr * db\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "Ys-O-KAW2ACj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Loop**\n",
        "\n",
        "### This loop repeatedly predicts, calculates loss, and updates weights for a set number of epochs. We use a very small learning rate because salary values are large."
      ],
      "metadata": {
        "id": "t4vvCgQCzM6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "lr = 0.0001 # Small learning rate to maintain stability with large numeric values\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # 1. Forward pass\n",
        "    y_hat = predict(X, w, b)\n",
        "\n",
        "    # 2. Compute loss\n",
        "    loss = mean_squared_error(y, y_hat)\n",
        "\n",
        "    # 3. Compute gradients\n",
        "    dw, db = compute_gradients(X, y, y_hat)\n",
        "\n",
        "    # 4. Update weights and bias\n",
        "    w, b = update_parameters(w, b, dw, db, lr)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Think About This (Step 8):\n",
        "# - Loss trend: Should decrease as the model learns\n",
        "# - If loss increases: The learning rate is too high, causing the model to overshoot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXXSd0Rh2Kxq",
        "outputId": "cafc9a34-a1e1-4302-9599-a27a932ce958"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1727049635.0\n",
            "Epoch 100, Loss: 66491868.55311352\n",
            "Epoch 200, Loss: 61752567.201190114\n",
            "Epoch 300, Loss: 58616531.07847049\n",
            "Epoch 400, Loss: 56528801.53951118\n",
            "Epoch 500, Loss: 55126542.02946697\n",
            "Epoch 600, Loss: 54172526.94885703\n",
            "Epoch 700, Loss: 53511656.14292054\n",
            "Epoch 800, Loss: 53042523.72795741\n",
            "Epoch 900, Loss: 52698829.56325033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Evaluation**\n",
        "\n",
        "### After training, we check the final weights and use the model to predict the salary of a new candidate."
      ],
      "metadata": {
        "id": "nCWW6obPzYt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Final Weights (age, experience): {w}\")\n",
        "print(f\"Final Bias: {b}\")\n",
        "\n",
        "# Example prediction:\n",
        "new_candidate = np.array([50, 15])\n",
        "predicted_income = new_candidate.dot(w) + b\n",
        "print(f\"Predicted income for [50, 15]: {predicted_income}\")\n",
        "\n",
        "# Think About This (Step 9):\n",
        "# - Reasonable?: Yes, if it aligns with the data trends\n",
        "# - Better than threshold rules?: Yes, because it handles magnitude and provides precise estimates"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeeT9owh2VzP",
        "outputId": "b389343e-3644-45ce-8255-2b525a472db0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights (age, experience): [ 764.75405919 1371.03430441]\n",
            "Final Bias: 321.73641174472493\n",
            "Predicted income for [50, 15]: 59124.953937381215\n"
          ]
        }
      ]
    }
  ]
}